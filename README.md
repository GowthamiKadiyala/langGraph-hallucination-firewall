ğŸ›¡ï¸ AI Hallucination Firewall

A self-correcting LLM workflow built using LangGraph + LangChain to reduce hallucinations in AI-generated responses.

Instead of trusting a single model output, this system validates, scores, and conditionally rewrites responses using a structured state-machine architecture.

ğŸš€ Why This Project?

Large Language Models can generate confident but incorrect information (hallucinations).

In production systems, especially in regulated domains (finance, healthcare, legal), blindly trusting LLM outputs is risky.

This project demonstrates how to move from simple prompt engineering to workflow engineering, where the system:

Generates

Validates

Scores

Self-corrects

before returning a response.

ğŸ§  How It Works

The system follows a multi-step validation pipeline:

User Question
      â†“
Generate Answer
      â†“
Extract Factual Claims
      â†“
Verify Claims
      â†“
Score Hallucination Risk
      â†“
If Risk High â†’ Rewrite â†’ Revalidate
If Risk Low  â†’ Return Final Answer


The process repeats until:

The answer meets a safety threshold

Or a maximum retry limit is reached

ğŸ—ï¸ Architecture

The workflow is implemented as a state machine using LangGraph.

Core Nodes

Generator â€“ Produces initial LLM answer

Extractor â€“ Breaks answer into factual claims

Verifier â€“ Validates each claim independently

Scorer â€“ Calculates hallucination risk score

Rewriter â€“ Regenerates answer if risk exceeds threshold

Risk Calculation
Risk = 1 - (Verified Claims / Total Claims)


Higher risk â†’ higher probability of hallucination.

âš™ï¸ Why LangGraph?

Supports shared state across steps

Enables conditional routing

Allows retry loops

Prevents infinite execution with iteration caps

Makes complex AI workflows clean and modular

LangChain is used for LLM execution.
LangGraph orchestrates the workflow.

This separation improves scalability and maintainability.

ğŸ› ï¸ Tech Stack

Python 3.12+

LangChain

LangGraph

langchain-openai

OpenAI API

python-dotenv
